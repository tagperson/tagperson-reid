# encoding: utf-8
"""
"""

import copy
import itertools
from collections import defaultdict
from typing import Optional
import random

import numpy as np
from torch.utils.data.sampler import Sampler

from fastreid.utils import comm


def no_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j != b]


def reorder_index(batch_indices, world_size):
    r"""Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:

    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


class CameraBalanceSampler(Sampler):
    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, num_cams_per_batch: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances
        self.num_cams_per_batch = num_cams_per_batch

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size
        self.mini_batch_size = mini_batch_size

        self.index_pid = dict()
        self.pid_index = defaultdict(list)
        self.pid_cam = defaultdict(list)
        
        self.cam_index_dic = dict()
        self.num_pids_per_cam = self.num_pids_per_batch // self.num_cams_per_batch
        if self.num_pids_per_cam <= 0:
            raise ValueError(f"num_pids_per_cam <= 0")

        for index, info in enumerate(data_source):
            pid = info[1]
            camid = info[2]
            
            self.index_pid[index] = pid
            self.pid_cam[pid].append(camid)
            self.pid_index[pid].append(index)

            if camid not in self.cam_index_dic.keys():
                self.cam_index_dic[camid]=defaultdict(list)
            self.cam_index_dic[camid][pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            batch_indices = []
            cams = np.random.choice(list(self.cam_index_dic.keys()), size=self.num_cams_per_batch, replace=True)
            for c in cams:
                pids = np.random.choice(list(self.cam_index_dic[c].keys()), size=self.num_pids_per_cam, replace=True)
                for p in pids:
                    idxs = np.random.choice(self.cam_index_dic[c][p], size=self.num_instances,replace=True)
                    random.shuffle(idxs)
                    batch_indices.extend(idxs)

            if len(batch_indices) == self.mini_batch_size:
                yield from reorder_index(batch_indices, self._world_size)
                batch_indices = []

